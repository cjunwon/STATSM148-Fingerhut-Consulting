{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 12 - Fingerhut FreshStart Customer Behavior Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Basic Libraries (specific libraries imported later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import mchmm as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pydtmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "fingerhut = pd.read_csv('export.csv')\n",
    "event_def = pd.read_csv('event_definitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rows have event_name == \"order_shipped\"\n",
    "\n",
    "len(fingerhut[fingerhut['event_name'] == 'order_shipped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(event_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of unique values in each column\n",
    "\n",
    "fingerhut.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fingerhut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values in 'customer_id' column\n",
    "print(fingerhut['customer_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values in 'customer_id' column\n",
    "print(fingerhut['customer_id'].nunique())\n",
    "\n",
    "# number of unique values in 'account_id' column\n",
    "print(fingerhut['account_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_customer_grouped = fingerhut.groupby('account_id')['customer_id'].nunique()\n",
    "\n",
    "sum(account_customer_grouped > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_account_grouped = fingerhut.groupby('customer_id')['account_id'].nunique()\n",
    "\n",
    "sum(customer_account_grouped > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find earliest and latest dates in 'event_timestamp' column\n",
    "\n",
    "print(fingerhut['event_timestamp'].min())\n",
    "print(fingerhut['event_timestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut['event_timestamp'] = pd.to_datetime(fingerhut['event_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fingerhut['event_timestamp'].groupby(pd.to_datetime(fingerhut['event_timestamp']).dt.year).agg('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut['event_timestamp'].groupby(pd.to_datetime(fingerhut['event_timestamp']).dt.year).agg('count').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fingerhut['event_timestamp'].groupby(pd.to_datetime(fingerhut['event_timestamp']).dt.month).agg('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract number of rows for each month in 'event_timestamp' column, and create plot of results\n",
    "\n",
    "fingerhut['event_timestamp'].groupby(pd.to_datetime(fingerhut['event_timestamp']).dt.month).agg('count').plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat above for just 2021 and 2022:\n",
    "\n",
    "fingerhut_2021_2022 = fingerhut[(fingerhut['event_timestamp'] >= '2021-01-01') & (fingerhut['event_timestamp'] < '2023-01-01')]\n",
    "\n",
    "print(fingerhut_2021_2022['event_timestamp'].groupby(fingerhut_2021_2022['event_timestamp'].dt.month).agg('count'))\n",
    "\n",
    "fingerhut_2021_2022['event_timestamp'].groupby(fingerhut_2021_2022['event_timestamp'].dt.month).agg('count').plot(kind='line')\n",
    "plt.xlabel('Total number of events by month (2021-2022)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut_2021_2022.resample('M', on = 'event_timestamp').size().plot()\n",
    "plt.xlabel('Total number of events by month (2021-2022)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max journey steps value for each unique customer, and store values in list\n",
    "\n",
    "unique_accounts = fingerhut['account_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_journey_steps = fingerhut.groupby('account_id')['journey_steps_until_end'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot of max journey steps values\n",
    "\n",
    "max_journey_steps.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring journey_steps_until_end, find all duplicate rows\n",
    "\n",
    "figerhut_no_journey_steps_until_end = fingerhut.drop(columns=['journey_steps_until_end'])\n",
    "\n",
    "duplicates = figerhut_no_journey_steps_until_end[figerhut_no_journey_steps_until_end.duplicated()]\n",
    "\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut_copy = fingerhut.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove fingerhut from memory\n",
    "\n",
    "del fingerhut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding number of customer_ids per account_id\n",
    "account_customer_grouped = fingerhut_copy.groupby('account_id')['customer_id'].nunique()\n",
    "\n",
    "# return only the accounts with more than one customer\n",
    "account_customer_grouped = account_customer_grouped[account_customer_grouped > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding number of account_ids per customer_id\n",
    "customer_account_grouped = fingerhut_copy.groupby('customer_id')['account_id'].nunique()\n",
    "\n",
    "# return only the customers with more than one account\n",
    "customer_account_grouped = customer_account_grouped[customer_account_grouped > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete journey_steps_until_end column\n",
    "\n",
    "del fingerhut_copy['journey_steps_until_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "\n",
    "fingerhut_copy = fingerhut_copy[~fingerhut_copy.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the accounts with more than one customer\n",
    "\n",
    "fingerhut_many_customers = fingerhut_copy[fingerhut_copy['account_id'].isin(account_customer_grouped.index)]\n",
    "\n",
    "# pull out the customers with more than one account\n",
    "\n",
    "fingerhut_many_accounts = fingerhut_copy[fingerhut_copy['customer_id'].isin(customer_account_grouped.index)]\n",
    "\n",
    "# take out fingerhut_many_customers and fingerhut_many_accounts from fingerhut_copy\n",
    "\n",
    "fingerhut_copy = fingerhut_copy[~fingerhut_copy['account_id'].isin(account_customer_grouped.index)]\n",
    "fingerhut_copy = fingerhut_copy[~fingerhut_copy['customer_id'].isin(customer_account_grouped.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index for all three dataframes\n",
    "\n",
    "fingerhut_copy.reset_index(drop=True, inplace=True)\n",
    "fingerhut_many_customers.reset_index(drop=True, inplace=True)\n",
    "fingerhut_many_accounts.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in fingerhut_copy called 'combined_id' starting at 0 and incrementing by 1 for each new account_id\n",
    "\n",
    "fingerhut_copy['combined_id'] = fingerhut_copy.groupby('account_id').ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort fingerhut_copy by combined_id\n",
    "\n",
    "fingerhut_copy.sort_values(by=['combined_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in fingerhut_many_customers called 'combined_id' starting at fingerhut_copy['combined_id'].max()\n",
    "\n",
    "fingerhut_many_customers['combined_id'] = fingerhut_many_customers.groupby('account_id').ngroup() + fingerhut_copy['combined_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort fingerhut_many_customers by combined_id\n",
    "\n",
    "fingerhut_many_customers.sort_values(by='combined_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in fingerhut_many_accounts called 'combined_id' starting at fingerhut_many_customers['combined_id'].max()\n",
    "\n",
    "fingerhut_many_accounts['combined_id'] = fingerhut_many_accounts.groupby('customer_id').ngroup() + fingerhut_many_customers['combined_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort fingerhut_many_accounts by combined_id\n",
    "\n",
    "fingerhut_many_accounts.sort_values(by='combined_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all three dataframes together\n",
    "\n",
    "fingerhut_combined = fingerhut_copy.append(fingerhut_many_customers)\n",
    "fingerhut_combined = fingerhut_combined.append(fingerhut_many_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combined_id, sort by event_timestamp\n",
    "\n",
    "fingerhut_combined.sort_values(by=['combined_id', 'event_timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of fingerhut_combined\n",
    "\n",
    "fingerhut_combined.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every combined_id, add a column called 'journey_steps_until_end' that increments by 1 for each row\n",
    "\n",
    "fingerhut_combined['journey_steps_until_end'] = fingerhut_combined.groupby('combined_id').cumcount(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export fingerhut_combined to csv\n",
    "\n",
    "fingerhut_combined.to_csv('fingerhut_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut_combined = pd.read_csv('fingerhut_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_definitions = pd.read_csv('event_definitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary based on 'stage' and 'event_definition_id' columns in event_definitions, using 'event_definition_id' as the key and 'stage' as the value\n",
    "\n",
    "event_dict = event_definitions.set_index('event_definition_id')['stage'].to_dict()\n",
    "\n",
    "# add additional key value pair\n",
    "event_dict[1] = 'Promotion Created'\n",
    "event_dict[24] = 'Campaignemail Clicked'\n",
    "\n",
    "event_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append a new column to fingerhut_combined called 'stage' that contains the value from the dictionary based on the 'event_name' column\n",
    "\n",
    "fingerhut_combined['stage'] = fingerhut_combined['ed_id'].map(event_dict)\n",
    "\n",
    "fingerhut_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of rows where 'stage' is NaN\n",
    "\n",
    "fingerhut_combined['stage'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_characteristics = pd.DataFrame(fingerhut_combined['combined_id'].unique(), columns=['combined_id'])\n",
    "\n",
    "customer_characteristics['application'] = np.nan\n",
    "customer_characteristics['activation'] = np.nan\n",
    "customer_characteristics['promotion_exposure'] = np.nan\n",
    "customer_characteristics['place_order_web'] = np.nan\n",
    "customer_characteristics['place_order_phone'] = np.nan\n",
    "customer_characteristics['order_shipped'] = np.nan\n",
    "\n",
    "customer_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any customers who do not have the 'Apply for Credit' stage in their journey, using groupby:\n",
    "\n",
    "credit_applications = fingerhut_combined.groupby('combined_id')['stage'].apply(lambda x: 'Apply for Credit' in x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(credit_applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on credit applications modify the customer_characteristics dataframe to add 0 to the 'application' column for customers who do not have the 'Apply for Credit' stage in their journey. Else add 1.\n",
    "\n",
    "customer_characteristics['application'] = customer_characteristics['combined_id'].map(credit_applications)\n",
    "\n",
    "customer_characteristics['application'] = customer_characteristics['application'].astype(int)\n",
    "\n",
    "customer_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotion_ids = [2, 9, 20, 21, 1, 24]\n",
    "\n",
    "# check if there are any customers who have not been exposed to any promotions, using groupby (has at least one promotion_id as ed_id):\n",
    "\n",
    "promotion_exposure = fingerhut_combined.groupby('combined_id')['ed_id'].apply(lambda x: any(i in promotion_ids for i in x.values))\n",
    "\n",
    "sum(promotion_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on promotion exposure modify the customer_characteristics dataframe to add 1 to the 'promotion_exposure' column for customers who have not been exposed to any promotions. Else add 0.\n",
    "\n",
    "customer_characteristics['promotion_exposure'] = customer_characteristics['combined_id'].map(promotion_exposure)\n",
    "\n",
    "customer_characteristics['promotion_exposure'] = customer_characteristics['promotion_exposure'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING SYSTEM-WIDE TYPO\n",
    "\n",
    "# change all values 'account_activitation' to 'account_activation' in the 'event_name' column\n",
    "\n",
    "fingerhut_combined['event_name'] = fingerhut_combined['event_name'].replace('account_activitation', 'account_activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if event_name 'account_activitation' is in the journey of each customer using groupby:\n",
    "\n",
    "account_activations = fingerhut_combined.groupby('combined_id')['event_name'].apply(lambda x: 'account_activation' in x.values)\n",
    "\n",
    "account_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(account_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on account_activations modify the customer_characteristics dataframe to add 0 to the 'activation' column for customers who do not have the 'account_activation' stage in their journey. Else add 1.\n",
    "\n",
    "customer_characteristics['activation'] = customer_characteristics['combined_id'].map(account_activations)\n",
    "\n",
    "customer_characteristics['activation'] = customer_characteristics['activation'].astype(int)\n",
    "\n",
    "customer_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if 'place_order_phone' or 'place_order_web' is in the journey of each customer using groupby:\n",
    "\n",
    "place_orders_web = fingerhut_combined.groupby('combined_id')['event_name'].apply(lambda x: 'place_order_web' in x.values)\n",
    "\n",
    "place_orders_phone = fingerhut_combined.groupby('combined_id')['event_name'].apply(lambda x: 'place_order_phone' in x.values)\n",
    "\n",
    "sum(place_orders_web)\n",
    "sum(place_orders_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on place_orders modify the customer_characteristics dataframe to add 0 to the 'place_order' column for customers who do not have the 'place_order_phone' or 'place_order_web' stage in their journey. Else add 1.\n",
    "\n",
    "customer_characteristics['place_order_web'] = customer_characteristics['combined_id'].map(place_orders_web)\n",
    "customer_characteristics['place_order_phone'] = customer_characteristics['combined_id'].map(place_orders_phone)\n",
    "\n",
    "customer_characteristics['place_order_web'] = customer_characteristics['place_order_web'].astype(int)\n",
    "customer_characteristics['place_order_phone'] = customer_characteristics['place_order_phone'].astype(int)\n",
    "\n",
    "customer_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if 'order_shipped' is in the journey of each customer using groupby:\n",
    "\n",
    "order_shipped = fingerhut_combined.groupby('combined_id')['event_name'].apply(lambda x: 'order_shipped' in x.values)\n",
    "\n",
    "sum(order_shipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on order_shipped modify the customer_characteristics dataframe to add 0 to the 'order_shipped' column for customers who do not have the 'order_shipped' stage in their journey. Else add 1.\n",
    "\n",
    "customer_characteristics['order_shipped'] = customer_characteristics['combined_id'].map(order_shipped)\n",
    "\n",
    "customer_characteristics['order_shipped'] = customer_characteristics['order_shipped'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the customer_characteristics dataframe as a pickle file\n",
    "\n",
    "customer_characteristics.to_pickle('customer_characteristics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby 'combined_id' and store the 'ed_id' and 'stage' as a list in columns named 'ed_ids' and 'stages'\n",
    "\n",
    "fingerhut_combined_grouped = fingerhut_combined.groupby('combined_id').agg({'ed_id': list, 'stage': list, 'event_timestamp': list}).reset_index()\n",
    "\n",
    "fingerhut_combined_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import customer_characteristics.pkl\n",
    "\n",
    "customer_characteristics = pd.read_pickle('customer_characteristics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the fingerhut_combined_grouped dataframe with the customer_characteristics dataframe\n",
    "\n",
    "fingerhut_combined_grouped = pd.merge(fingerhut_combined_grouped, customer_characteristics, on='combined_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export fingerhut_combined_grouped to a pickle file int ../Dataset\n",
    "\n",
    "fingerhut_combined_grouped.to_pickle('fingerhut_combined_grouped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_df = pd.read_csv(\"event_definitions.csv\")\n",
    "df = pd.read_csv(\"export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_timestamp'] = pd.to_datetime(df['event_timestamp']) # convert to pd datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows - don't factor in journey_steps_until_end because this counts up even for duplicate rows\n",
    "df_dropped = df.drop(['journey_steps_until_end'], axis = 1)\n",
    "df_dropped = df_dropped.drop_duplicates()\n",
    "# df_dropped = df.drop(['journey_steps_until_end'], axis = 1)\n",
    "df_dropped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "df.shape[0] - df_dropped.shape[0] # number of duplicate rows removed ~8 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['journey_steps_until_end'] = df_dropped.groupby(['customer_id', 'account_id']).cumcount() + 1 # add journey_steps_until_end back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of account_activations for each customer/account pair\n",
    "activation_counts = df_dropped.groupby(['customer_id', 'account_id'])['event_name'].apply(lambda x: x.str.count('account_activitation').sum())\n",
    "activation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of account_activation occurrences\n",
    "activation_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get those with multiple activations\n",
    "multiple_activations = activation_counts[activation_counts > 1]\n",
    "multiple_activations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_act_cust = list(multiple_activations.index) # list of tuples of (customer_id, account_id) with multiple activations\n",
    "\n",
    "# create the filter for dataframe rows that correspond to customers with multiple activations\n",
    "filt = ~df_dropped.apply(lambda row: (row['customer_id'], row['account_id']) in mult_act_cust, axis=1)\n",
    "df_filtered = df_dropped[filt] # remove customers with multiple activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make sure all remaining customer/account combinations have only 0 or 1 account_activations\n",
    "df_filtered.groupby(['customer_id', 'account_id'])['event_name'].apply(lambda x: x.str.count('account_activitation').sum()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['customer_id', 'account_id']].drop_duplicates().shape # 1735767 unique combinations of customer/account id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2173 / 1735767) # 0.001% of customer/account combinations would be removed if we removed those with multiple account activations\n",
    "238751 / df_dropped.shape[0] # 0.4% of dataset would be dropped if we removed customers with multiple activations\n",
    "# It seems that multiple account activations just indicates multiple journeys under the same customer_id and account_id which is interesting\n",
    "\n",
    "# Because such a small portion of the dataset has multiple activations, we will drop these rows for simplicity and for the sake of keeping the data uniform.\n",
    "\n",
    "# We will proceed with attempting to remove \"incomplete\" customers in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Before proceeding with turning data into wide format, first add 'stage' column based on definition df\n",
    "\n",
    "stage_dict = {'Apply for Credit' : 1, 'Credit Account' : 2, 'Discover' : 3, 'Downpayment' : 4, 'First Purchase' : 5, \n",
    "              'Order Shipped' : 6, 'Prospecting' : 7} # create dict to map stage to int\n",
    "def_df['stage_int'] = def_df['stage'].map(stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_df.sort_values('event_definition_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = def_df.set_index('event_definition_id')['stage'].to_dict()\n",
    "\n",
    "# these are missing, so manually assign them\n",
    "event_dict[1] = 'Promotion Created'\n",
    "event_dict[24] = 'Campaignemail Clicked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['stage'] = df_filtered['ed_id'].map(event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['stage'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'customer_id' and aggregate the other columns into lists\n",
    "df_grouped_cust_acct = df_filtered.groupby(['customer_id', 'account_id']).agg({\n",
    "    'ed_id': list,\n",
    "    'event_name': list,\n",
    "    'event_timestamp': list,\n",
    "    'journey_steps_until_end': list,\n",
    "    'stage' : list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## add rows on account activation status\n",
    "\n",
    "# 1 if row activated account, 0 if not\n",
    "df_grouped_cust_acct['account_activation'] = df_grouped_cust_acct['event_name'].apply(lambda x: 1 if 'account_activitation' in x else 0)\n",
    "\n",
    "# 1 if row placed order, 0 if did not\n",
    "df_grouped_cust_acct['place_order'] = df_grouped_cust_acct['event_name'].apply(lambda x: 1 if any([i in x for i in ['place_order_web', 'place_order_phone']]) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_no_order = df_grouped_cust_acct[(df_grouped_cust_acct['account_activation'] == 1) & (df_grouped_cust_acct['place_order'] == 0)][['customer_id', 'account_id']]\n",
    "activated_no_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now proceed with removing incomplete customers - those who activated within the last 60 days\n",
    "\n",
    "cutoff_date = df_dropped['event_timestamp'].max() # extract last date in dataset\n",
    "\n",
    "activation_events = df_filtered[df_filtered['event_name'] == 'account_activitation'] # get account activations\n",
    "\n",
    "# Merge to keep only activation events where the order wasn't shipped\n",
    "df_merged = activation_events.merge(activated_no_order, on=['customer_id', 'account_id'])\n",
    "\n",
    "# Group by 'customer_id' and 'account_id' to find the latest activation timestamp for each\n",
    "activation_times = df_merged.groupby(['customer_id', 'account_id'])['event_timestamp'].max().reset_index()\n",
    "\n",
    "# Calculate the number of days since activation for each customer/account pair\n",
    "activation_times['days_since_activation'] = (cutoff_date - activation_times['event_timestamp']).dt.days\n",
    "\n",
    "# Filter out rows where 'days_since_activation' is greater than 60\n",
    "incomplete_cust_df = activation_times[activation_times['days_since_activation'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_cust = set(zip(incomplete_cust_df['customer_id'], incomplete_cust_df['account_id']))\n",
    "cleaned_df = df_grouped_cust_acct[~df_grouped_cust_acct.apply(lambda row: (row['customer_id'], row['account_id']) in incomplete_cust, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_pickle(\"cleaned_wide_format_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sankey Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "event_def = pd.read_csv('event_definitions.csv')\n",
    "fingerhut_combined = pd.read_csv('fingerhut_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    'Apply for Credit': [15, 17, 12, 14, 3, 19],\n",
    "    'Account Activation': [29],\n",
    "    'Fraud Review': [37],\n",
    "    'Promotion and Discover': [1, 2, 9, 10, 22, 23, 20, 21, 24],\n",
    "    'Downpayment': [27, 26, 8, 25],\n",
    "    'Shopping': [11, 6, 4, 5],\n",
    "    'Place Order': [18, 7],\n",
    "    'Order Shipped': [28]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge event_def (event_definition_id column) with fingerhut_combined (ed_id column)\n",
    "\n",
    "fingerhut_combined = pd.merge(fingerhut_combined, event_def, how='left', left_on='ed_id', right_on='event_definition_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max values from 'combined_id' column\n",
    "\n",
    "min_combined_id = fingerhut_combined['combined_id'].min()\n",
    "max_combined_id = fingerhut_combined['combined_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 1000 numbers from 1 to 1665430\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(12)\n",
    "\n",
    "random_numbers = random.sample(range(1, 1665430), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where combined_id is in random_numbers\n",
    "\n",
    "fingerhut_small = fingerhut_combined[fingerhut_combined['combined_id'].isin(random_numbers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby combined_id and only keep groups that has at least one instance of ed_id == 29 (account_activation) and does not have any instance of ed_id == 16 (application_phone_declined) or ed_id == 13 (application_web_declined)\n",
    "\n",
    "fingerhut_active_account = fingerhut_small.groupby('combined_id').filter(lambda x: (x['ed_id'] == 29).any() & (x['ed_id'] != 16).all() & (x['ed_id'] != 13).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep 'combined_id', 'event_timestamp', 'event_name_y'\n",
    "\n",
    "fingerhut_active_account = fingerhut_active_account[['combined_id', 'ed_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the event_dict to create a new column called 'event_type' in fingerhut_active_account. The value of 'event_type' should be the key of the event_dict, which has lists of ed_id as values.\n",
    "\n",
    "fingerhut_active_account['event_type'] = fingerhut_active_account['ed_id'].map({v: k for k, l in event_dict.items() for v in l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ed_id column\n",
    "\n",
    "fingerhut_active_account = fingerhut_active_account.drop(columns=['ed_id'])\n",
    "\n",
    "# remove duplicates\n",
    "\n",
    "fingerhut_active_account = fingerhut_active_account.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby combined_id, and if the last event_type is not 'Order Shipped', then add new row with event_type 'No Order Made', and also add the combined_id to the new row\n",
    "\n",
    "fingerhut_active_account = fingerhut_active_account.groupby('combined_id').apply(lambda x: x.append({'event_type': 'No Order Made', 'combined_id': x['combined_id'].iloc[0]}, ignore_index=True) if x['event_type'].iloc[-1] != 'Order Shipped' else x)\n",
    "\n",
    "# reset index\n",
    "\n",
    "fingerhut_active_account = fingerhut_active_account.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NA values\n",
    "\n",
    "fingerhut_active_account.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut_active_account['sequence'] = fingerhut_active_account.groupby('combined_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fingerhut_active_account = fingerhut_active_account.pivot(index='combined_id', columns='sequence', values='event_type').reset_index(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_fingerhut_active_account = pivot_fingerhut_active_account.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sankey_chart_data(df: pd.DataFrame):\n",
    "    # list of list: each list is the set of nodes in each tier/column\n",
    "    column_values = [df[col] for col in df.columns]\n",
    "\n",
    "    # this generates the labels for the sankey by taking all the unique values\n",
    "    labels = sum([list(node_values.unique()) for node_values in column_values], [])\n",
    "\n",
    "    # initializes a dict of dicts (one dict per tier)\n",
    "    link_mappings = {col: {} for col in df.columns}\n",
    "\n",
    "    # each dict maps a node to a unique number value\n",
    "    i = 0\n",
    "    for col, nodes in zip(df.columns, column_values):\n",
    "        for node in nodes.unique():\n",
    "            link_mappings[col][node] = i\n",
    "            i += 1\n",
    "\n",
    "    # specifying which columns are serving as sources and which as targets\n",
    "    source_cols = df.columns[:-1]\n",
    "    target_cols = df.columns[1:]\n",
    "    links = []\n",
    "\n",
    "    # loop to create a list of links in the format [((src, tgt), wt), (), ()...]\n",
    "    for source_col, target_col in zip(source_cols, target_cols):\n",
    "        for source, target in zip(df[source_col], df[target_col]):\n",
    "            links.append(\n",
    "                (\n",
    "                    link_mappings[source_col][source],\n",
    "                    link_mappings[target_col][target],\n",
    "                    1  # Weight is 1 for counting transitions\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # creating a dataframe with 3 columns: source, target, and weight\n",
    "    df_links = pd.DataFrame(links, columns=[\"source\", \"target\", \"weight\"])\n",
    "\n",
    "    # generating three lists needed for the sankey visual\n",
    "    sources = df_links[\"source\"]\n",
    "    targets = df_links[\"target\"]\n",
    "    weights = df_links[\"weight\"]\n",
    "\n",
    "    return labels, sources, targets, weights\n",
    "\n",
    "\n",
    "# Your DataFrame\n",
    "df = pivot_fingerhut_active_account.iloc[:, 2:]\n",
    "\n",
    "# Call the generate_sankey_chart_data function\n",
    "labels, sources, targets, weights = generate_sankey_chart_data(df=df)\n",
    "\n",
    "\n",
    "# Map colors to labels\n",
    "label_colors = {\n",
    "    'Promotion and Discover': 'pink',\n",
    "    'Apply for Credit': 'green',\n",
    "    'Shopping': 'blue',\n",
    "    'Account Activation': 'cyan',\n",
    "    'Downpayment': 'purple',\n",
    "    'Place Order': 'yellow',\n",
    "    'Order Shipped': 'orange',\n",
    "    'No Order Made': 'red',\n",
    "    '': 'black'  # Adjust this for labels with empty strings\n",
    "}\n",
    "\n",
    "# Create a DataFrame to aggregate weights\n",
    "df_links_aggregated = pd.DataFrame({'source': sources, 'target': targets, 'weight': weights})\n",
    "\n",
    "# Aggregate weights for the same source and target pairs\n",
    "df_links_aggregated = df_links_aggregated.groupby(['source', 'target'], as_index=False).agg({'weight': 'sum'})\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels,\n",
    "        color=[label_colors[label] for label in labels]  # Map colors based on the dictionary\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=df_links_aggregated['source'],\n",
    "        target=df_links_aggregated['target'],\n",
    "        value=df_links_aggregated['weight'],\n",
    "        # Add labels for each link (source to target) with the total counts\n",
    "        label=[f\"Total Counts: {weight}\" for weight in df_links_aggregated['weight']]\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Journey flow of customers with account activation (237 random customers)\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle = pd.read_pickle('fingerhut_combined_grouped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'prospecting' is not present in any of the lists in the 'stage' column\n",
    "prospecting_absent = all(pickle['stage'].apply(lambda stages: 'prospecting' not in stages))\n",
    "\n",
    "if prospecting_absent:\n",
    "    print(\"Confirmed: 'prospecting' is not present in any of the stage lists.\")\n",
    "else:\n",
    "    print(\"Warning: 'prospecting' was found in one or more of the stage lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_def = pd.read_csv('Event_Definitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_def['stage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(31524)\n",
    "medium_pickle = pickle.sample(n=16000, random_state = 31524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chains = []\n",
    "\n",
    "for index, row in medium_pickle.iterrows():\n",
    "\n",
    "    sequence = row['stage']\n",
    "\n",
    "    mc_obj = mc.MarkovChain().from_data(sequence)\n",
    "\n",
    "    markov_chains.append(mc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = set()\n",
    "for mc in markov_chains:\n",
    "    unique_states.update(mc.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an aggregated transition count matrix\n",
    "n = len(unique_states)\n",
    "aggregated_counts = np.zeros((n, n))\n",
    "\n",
    "# Map each state to its index in the aggregated matrix\n",
    "state_to_index = {state: i for i, state in enumerate(unique_states)}\n",
    "\n",
    "for mc in markov_chains:\n",
    "    # Increment counts in the aggregated matrix based on observed transitions in mc\n",
    "    for i in range(len(mc.states)-1):\n",
    "        from_state, to_state = mc.states[i], mc.states[i+1]\n",
    "        from_index, to_index = state_to_index[from_state], state_to_index[to_state]\n",
    "        aggregated_counts[from_index, to_index] += 1  # Increment count for observed transition\n",
    "\n",
    "# Optional: Convert counts to probabilities by normalizing each row\n",
    "aggregated_probs = aggregated_counts / aggregated_counts.sum(axis=1, keepdims=True)\n",
    "#aggregated_probs = np.nan_to_num(aggregated_probs)  # Handle division by zero for states with no outgoing transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(aggregated_probs, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            xticklabels=unique_states, yticklabels=unique_states)\n",
    "plt.title(\"Aggregated Transition Probabilities\")\n",
    "plt.xlabel(\"To State\")\n",
    "plt.ylabel(\"From State\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(aggregated_counts, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            xticklabels=unique_states, yticklabels=unique_states)\n",
    "plt.title(\"Aggregated Transition Probabilities\")\n",
    "plt.xlabel(\"To State\")\n",
    "plt.ylabel(\"From State\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_sample = pd.read_csv('smaller_sample.csv')\n",
    "smaller_sample.sort_values(by=['customer_id', 'event_timestamp'], inplace=True)\n",
    "unique_events = smaller_sample['event_name'].unique()\n",
    "n_events = len(unique_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name_to_index = {event_name: index for index, event_name in enumerate(unique_events)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = np.zeros((n_events, n_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group in smaller_sample.groupby('customer_id'):\n",
    "    events = group['event_name'].apply(lambda x: event_name_to_index[x]).values\n",
    "    for i in range(len(events) - 1):\n",
    "        current_event, next_event = events[i], events[i + 1]\n",
    "        transition_counts[current_event, next_event] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = transition_counts / transition_counts.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs[np.isnan(transition_probs)] = 0\n",
    "np.fill_diagonal(transition_probs, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df = pd.DataFrame(transition_probs, index=unique_events, columns=unique_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = transition_df.sum(axis=0)\n",
    "zero_columns = column_sums == 0\n",
    "\n",
    "# For columns that sum to 0, assign uniform probabilities\n",
    "for col in transition_df.columns[zero_columns]:\n",
    "    transition_df[col] = 1 / len(transition_df.columns)\n",
    "\n",
    "transition_df = transition_df.div(transition_df.sum(axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = transition_df.sum(axis=0)\n",
    "print(column_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = transition_df.index.tolist()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = pydtmc.MarkovChain(np.transpose(transition_df), names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydtmc.plot_eigenvalues(mc, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(transition_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Transition Matrix Heatmap')\n",
    "plt.xlabel('To State')\n",
    "plt.ylabel('From State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in fingerhut_combined_grouped.pkl\n",
    "\n",
    "fingerhut_combined_grouped = pd.read_pickle('../Dataset/fingerhut_combined_grouped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if either the place_order_web or place_order_phone is 1, then make new column place_order = 1, else 0\n",
    "\n",
    "fingerhut_combined_grouped['place_order'] = np.where((fingerhut_combined_grouped['place_order_web'] == 1) | (fingerhut_combined_grouped['place_order_phone'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 100000 rows from fingerhut_combined_grouped\n",
    "\n",
    "# fingerhut_combined_grouped_sample = fingerhut_combined_grouped.sample(n=100000, random_state=0)\n",
    "\n",
    "fingerhut_combined_grouped_sample = fingerhut_combined_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have activation = 1\n",
    "\n",
    "fingerhut_combined_grouped_sample['activation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have place_order = 1 and activation = 1\n",
    "\n",
    "ordered_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['place_order'] == 1) & (fingerhut_combined_grouped_sample['activation'] == 1)]\n",
    "print(len(ordered_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have place_order = 0 and activation = 0\n",
    "\n",
    "no_ordered_no_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['place_order'] == 0) & (fingerhut_combined_grouped_sample['activation'] == 0)]\n",
    "print(len(no_ordered_no_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have place_order = 1 and activation = 0\n",
    "\n",
    "ordered_no_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['place_order'] == 1) & (fingerhut_combined_grouped_sample['activation'] == 0)]\n",
    "print(len(ordered_no_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have place_order = 0 and activation = 1\n",
    "\n",
    "no_ordered_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['place_order'] == 0) & (fingerhut_combined_grouped_sample['activation'] == 1)]\n",
    "print(len(no_ordered_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all the above 4 categories should be 100000\n",
    "\n",
    "len(ordered_activated) + len(no_ordered_no_activated) + len(ordered_no_activated) + len(no_ordered_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have promotion_exposure = 1\n",
    "\n",
    "fingerhut_combined_grouped_sample['promotion_exposure'].value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have promotion_expousre = 1 and activation = 1\n",
    "\n",
    "promotion_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['promotion_exposure'] == 1) & (fingerhut_combined_grouped_sample['activation'] == 1)]\n",
    "print(len(promotion_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows have promotion_expousre = 1 and activation = 0\n",
    "\n",
    "promotion_no_activated = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['promotion_exposure'] == 1) & (fingerhut_combined_grouped_sample['activation'] == 0)]\n",
    "print(len(promotion_no_activated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotion_ids = [2, 9, 20, 21, 1, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row, if the list object in the ed_id column contains any of the above 6 ids, then make a new column `promotion_type` that contains the list of ids that were found in the ed_id column\n",
    "\n",
    "fingerhut_combined_grouped_sample['promotion_type'] = fingerhut_combined_grouped_sample['ed_id'].apply(lambda x: list(set(x).intersection(promotion_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the promotion_type column, create new columns for each of the 6 ids and set the value to 1 if the id is present in the promotion_type list, else 0\n",
    "\n",
    "for i in promotion_ids:\n",
    "    fingerhut_combined_grouped_sample[i] = fingerhut_combined_grouped_sample['promotion_type'].apply(lambda x: 1 if i in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any of the columns 2, 9, 20, 21, 1, 24 are 1, then make a new column `promotion_y_n` = 1, else 0\n",
    "\n",
    "fingerhut_combined_grouped_sample['promotion_y_n'] = np.where((fingerhut_combined_grouped_sample[2] == 1) | (fingerhut_combined_grouped_sample[9] == 1) | (fingerhut_combined_grouped_sample[20] == 1) | (fingerhut_combined_grouped_sample[21] == 1) | (fingerhut_combined_grouped_sample[1] == 1) | (fingerhut_combined_grouped_sample[24] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerhut_combined_grouped_sample[[2, 9, 20, 21, 1, 24, 'promotion_y_n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the activation column as the target variable, create a train-test split with 80% of the data in the training set and 20% in the test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = fingerhut_combined_grouped_sample.drop('activation', axis=1)\n",
    "# drop the columns that are not needed\n",
    "\n",
    "X = fingerhut_combined_grouped_sample[[2, 9, 20, 21, 1, 24, 'promotion_y_n']]\n",
    "X.columns = X.columns.astype(str)\n",
    "y = fingerhut_combined_grouped_sample['activation']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Using the training set, train a logistic regression model to predict the activation column. Use the following hyperparameters: max_iter=1000, random_state=0.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=0, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the logistic regression model\n",
    "\n",
    "logreg.coef_\n",
    "\n",
    "# Pair feature names with coefficients\n",
    "\n",
    "feature_names = X_train.columns\n",
    "feature_names\n",
    "\n",
    "feature_coefficients = dict(zip(feature_names, logreg.coef_[0]))\n",
    "\n",
    "feature_coefficients\n",
    "\n",
    "# Sort features by value of coefficient, in descending order\n",
    "\n",
    "sorted_features = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_features\n",
    "\n",
    "# exponentiate the coefficients to get the odds ratio\n",
    "\n",
    "odds_ratio = {k: np.exp(v) for k, v in feature_coefficients.items()}\n",
    "odds_ratio\n",
    "\n",
    "# order the odds ratio in descending order\n",
    "\n",
    "sorted_odds_ratio = sorted(odds_ratio.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_odds_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the test set, predict the activation column and calculate the accuracy of the model.\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the test set, calculate the confusion matrix and classification report of the model.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try cross-validation in case the model is overfitting\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "cv_scores\n",
    "\n",
    "# find mean of the cross-validation scores\n",
    "\n",
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promotion Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows where application = 1, activation = 1, place_order = 1\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered = fingerhut_combined_grouped_sample[(fingerhut_combined_grouped_sample['application'] == 1) & (fingerhut_combined_grouped_sample['activation'] == 1) & (fingerhut_combined_grouped_sample['place_order'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_for_credit_ids = [3, 12, 13, 14, 15, 16, 17, 19]\n",
    "account_activation_ids = [29]\n",
    "place_order_ids = [7, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ed_id is a column containing lists of ids for each row. event_timestamp is a column containing lists of the same length as ed_id.\n",
    "# For each row, extract the timestamp from the event_timestamp column that corresponds to the id in the ed_id column that matches the apply_for_credit_ids, account_activation_ids, and place_order_ids lists and store each timestamp in a new column respectively.\n",
    "\n",
    "# fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_timestamp'] = fingerhut_combined_grouped_sample_activated_ordered.apply(lambda x: x['event_timestamp'][x['ed_id'].index(3)] if 3 in x['ed_id'] else None, axis=1)\n",
    "fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_timestamp'] = fingerhut_combined_grouped_sample_activated_ordered.apply(lambda x: x['event_timestamp'][next((i for i, id in enumerate(x['ed_id']) if id in apply_for_credit_ids), None)] if any(id in x['ed_id'] for id in apply_for_credit_ids) else None, axis=1)\n",
    "fingerhut_combined_grouped_sample_activated_ordered['account_activation_timestamp'] = fingerhut_combined_grouped_sample_activated_ordered.apply(lambda x: x['event_timestamp'][x['ed_id'].index(29)] if 29 in x['ed_id'] else None, axis=1)\n",
    "# fingerhut_combined_grouped_sample_activated_ordered['place_order_timestamp'] = fingerhut_combined_grouped_sample_activated_ordered.apply(lambda x: x['event_timestamp'][x['ed_id'].index(7)] if 7 in x['ed_id'] else None, axis=1)\n",
    "fingerhut_combined_grouped_sample_activated_ordered['place_order_timestamp'] = fingerhut_combined_grouped_sample_activated_ordered.apply(lambda x: x['event_timestamp'][next((i for i, id in enumerate(x['ed_id']) if id in place_order_ids), None)] if any(id in x['ed_id'] for id in place_order_ids) else None, axis=1)\n",
    "\n",
    "# Using the timestamps from the previous step, calculate the time it took for each customer to go from applying for credit to activating their account, and from activating their account to placing an order.\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_timestamp'] = pd.to_datetime(fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_timestamp'])\n",
    "fingerhut_combined_grouped_sample_activated_ordered['account_activation_timestamp'] = pd.to_datetime(fingerhut_combined_grouped_sample_activated_ordered['account_activation_timestamp'])\n",
    "fingerhut_combined_grouped_sample_activated_ordered['place_order_timestamp'] = pd.to_datetime(fingerhut_combined_grouped_sample_activated_ordered['place_order_timestamp'])\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_to_activation'] = (fingerhut_combined_grouped_sample_activated_ordered['account_activation_timestamp'] - fingerhut_combined_grouped_sample_activated_ordered['apply_for_credit_timestamp']).dt.days\n",
    "fingerhut_combined_grouped_sample_activated_ordered['activation_to_place_order'] = (fingerhut_combined_grouped_sample_activated_ordered['place_order_timestamp'] - fingerhut_combined_grouped_sample_activated_ordered['account_activation_timestamp']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract apply_for_credit_to_activation and activation_to_place_order columns and save them to a new dataframe\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time = fingerhut_combined_grouped_sample_activated_ordered[['apply_for_credit_to_activation', 'activation_to_place_order']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both columns as histograms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time['apply_for_credit_to_activation'].plot(kind='hist', bins=60)\n",
    "# add title and axis labels\n",
    "\n",
    "plt.title('Days from Applying for Credit to Account Activation')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# fingerhut_combined_grouped_sample_activated_ordered_time['activation_to_place_order'].plot(kind='hist', bins=60)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append promotion_y_n from fingerhut_combined_grouped_sample to fingerhut_combined_grouped_sample_activated_ordered\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time['promotion_y_n'] = fingerhut_combined_grouped_sample_activated_ordered['promotion_y_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from apply_for_credit_to_activation column\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time = fingerhut_combined_grouped_sample_activated_ordered_time[fingerhut_combined_grouped_sample_activated_ordered_time['apply_for_credit_to_activation'] < 600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two histograms for apply_for_credit_to_activation based on whether promotion_y_n is 1 or 0\n",
    "\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time[fingerhut_combined_grouped_sample_activated_ordered_time['promotion_y_n'] == 1]['apply_for_credit_to_activation'].plot(kind='hist', bins=60, alpha=0.5, label='Promotion')\n",
    "fingerhut_combined_grouped_sample_activated_ordered_time[fingerhut_combined_grouped_sample_activated_ordered_time['promotion_y_n'] == 0]['apply_for_credit_to_activation'].plot(kind='hist', bins=60, alpha=0.5, label='No Promotion')\n",
    "\n",
    "plt.title('Days from Applying for Credit to Account Activation (outliers removed)')\n",
    "\n",
    "plt.xlabel('Days')\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - Successful Journeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"cleaned_wide_format_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_events = df['event_name'].apply(lambda x: x[0] if x else None)\n",
    "last_events = df['event_name'].apply(lambda x: x[-1] if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_events.value_counts().plot(kind = 'bar')\n",
    "plt.title(\"First Event Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_events.value_counts().plot(kind = 'bar')\n",
    "plt.title(\"Last Event Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_orders = df[df['event_name'].apply(lambda x: 'order_shipped' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_with_orders.shape)\n",
    "df_with_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_orders = df[df['event_name'].apply(lambda x: 'order_shipped' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_df = pd.read_csv(\"event_definitions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = {'Apply for Credit' : 1, 'Credit Account' : 2, 'Discover' : 3, 'Downpayment' : 4, 'First Purchase' : 5, \n",
    "              'Order Shipped' : 6, 'Prospecting' : 7}\n",
    "def_df['stage_int'] = def_df['stage'].map(stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_mapping_dict = def_df.set_index('event_definition_id')['stage_int'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(n=500000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.to_datetime('2023-09-20 12:29:58+0000', utc=True)\n",
    "cutoff_date # this is like \"today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_stages = {'Apply for Credit', 'First Purchase', 'Downpayment', 'Order Shipped'}\n",
    "promotion_ids = [2, 9, 20, 21, 1, 24]\n",
    "\n",
    "\n",
    "# Function to get the stage from the first id in the list\n",
    "def get_stage_from_first_id(id_list):\n",
    "    # Get the first id\n",
    "    first_id = id_list[0]\n",
    "    # Return the corresponding stage using the mapping dictionary\n",
    "    return stage_mapping_dict.get(first_id, 0)\n",
    "\n",
    "def get_stage_from_last_id(id_list):\n",
    "    # Get the first id\n",
    "    last_id = id_list[-1]\n",
    "    # Return the corresponding stage using the mapping dictionary\n",
    "    return stage_mapping_dict.get(last_id, 0)\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    # df['event_count'] = df['event_name'].apply(len)\n",
    "    # df['num_accounts'] = df['account_id'].apply(len)\n",
    "    # df['unique_event_count'] = df['event_name'].apply(lambda x: len(set(x)))\n",
    "    # df['unique_stage_count'] = df['stage'].apply(lambda x: len(set(x)))\n",
    "    \n",
    "    df['first_event'] = df['ed_id'].apply(lambda x: x[0] if x else None)\n",
    "    df['days_since_start'] = df['event_timestamp'].apply(lambda x: (cutoff_date - x[0]).days)\n",
    "    # df['last_event'] = df['ed_id'].apply(lambda x: x[-1] if x else None)\n",
    "    # df['len_journey'] = df['journey_steps_until_end'].apply(len)\n",
    "    # df['days_in_journey'] = df['event_timestamp'].apply(lambda x: (x[-1] - x[0]).days)\n",
    "    # df['progression_rate'] = df['len_journey'] / df['days_in_journey'] # estimate for progression rate of journey\n",
    "    df['first_stage'] = df['ed_id'].apply(get_stage_from_first_id)\n",
    "    # df['last_stage'] = df['ed_id'].apply(get_stage_from_last_id)\n",
    "\n",
    "    df['first_event_month'] = df['event_timestamp'].apply(lambda x: x[0].month)\n",
    "    df['first_event_day'] = df['event_timestamp'].apply(lambda x: x[0].day)\n",
    "    df['first_event_hour'] = df['event_timestamp'].apply(lambda x: x[0].hour)\n",
    "\n",
    "    # now create columns for whether a specific event is present or not\n",
    "    # remember that ideal journey is defined as: Apply for credit > Make a first purchase > Make the down payment > Order Ships\n",
    "    # df already has account activation and place order one hot encoded columns\n",
    "    # df['apply_for_credit'] = df['stage'].apply(lambda x: 1 if 'Apply for Credit' in x else 0) # Apply for credit\n",
    "    # df['make_first_purchase'] = df['stage'].apply(lambda x: 1 if 'First Purchase' in x else 0) # first purchase\n",
    "    # df['downpayment'] = df['stage'].apply(lambda x: 1 if 'Downpayment' in x else 0) # downpayment\n",
    "    # df['order_shipped'] = df['event_name'].apply(lambda x: 1 if 'order_shipped' in x else 0) # order shipped\n",
    "\n",
    "    df['ideal_journey'] = df['stage'].apply(lambda x: 1 if required_stages.issubset(set(x)) else 0)\n",
    "    df['promotion_exposure'] = df['ed_id'].apply(lambda x: 1 if any([i in x for i in promotion_ids]) else 0)\n",
    "\n",
    "\n",
    "    df = df.drop(['event_name', 'event_timestamp', 'ed_id', 'journey_steps_until_end', 'stage', 'place_order', 'account_activation'], axis = 1)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = clean_df(sampled_df)\n",
    "whole_sampled_df = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_pickle(\"updated_clustering_sample_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_sampled_df.isna().any() # check for NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_sampled_df.to_pickle((\"updated_feature_engineered_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = whole_sampled_df.drop(['customer_id', 'account_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(model_df.corr(), cmap='Blues', annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop(['ideal_journey'], axis = 1)\n",
    "y = whole_sampled_df['ideal_journey'] # we use ideal_journey rather than order_shipped etc because the two variables have a correlation of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "clf = LogisticRegression(max_iter = 10000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accurary', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall', recall)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['unsuccessful', 'successful']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now try with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000, class_weight = 'balanced').fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = clf.coef_[0]  \n",
    "feature_importance = zip(X_train.columns, coefficients)\n",
    "sorted_features = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "for feature, coef in sorted_features:\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = np.exp(coefficients)\n",
    "odds_df = pd.DataFrame({'Feature': X_train.columns, 'OddsRatio': odds_ratios})\n",
    "odds_df_sorted = odds_df.sort_values(by='OddsRatio', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "barplot = plt.barh(odds_df_sorted['Feature'], odds_df_sorted['OddsRatio'])\n",
    "for bar in barplot:\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, \n",
    "             f\"{bar.get_width():.2f}\", va='center')\n",
    "\n",
    "plt.xlabel('Odds Ratio')\n",
    "plt.title('Sorted Feature Importance (Odds Ratios) For Logistic Regression')\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try cross fold validation in case overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X, y, cv=10, scoring='accuracy') # 10 fold cv\n",
    "print(f'Accuracy for each fold: {scores}')\n",
    "print(f'Mean accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look into other models\n",
    "\n",
    "# first test on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = sampled_df.drop(['customer_id', 'account_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = temp.drop(['ideal_journey'], axis = 1)\n",
    "y_sample = temp['ideal_journey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_sample_scaled = scaler.fit_transform(X_sample)\n",
    "X_sample_scaled = pd.DataFrame(X_sample_scaled, columns=X_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_sample_scaled, y_sample, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000).fit(X_train0, y_train0)\n",
    "y_pred = clf.predict(X_test0)\n",
    "accuracy = accuracy_score(y_test0, y_pred)\n",
    "precision = precision_score(y_test0, y_pred)\n",
    "recall = recall_score(y_test0, y_pred)\n",
    "f1 = f1_score(y_test0, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accurary', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall', recall)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, \n",
    "                                       min_samples_leaf=2, max_features='sqrt', n_jobs=-1, \n",
    "                                       random_state=42, class_weight = 'balanced')\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('accurary', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall', recall)\n",
    "print('f1', f1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# setting class weight to balanced brough accuracy down from 82% to 64% but raised the other metrics significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, \n",
    "                                       min_samples_leaf=1, max_features='sqrt', n_jobs=-1, random_state=42)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(f'Accuracy for each fold: {scores}')\n",
    "print(f'Mean accuracy: {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter tuning for random forest classifier\n",
    "\n",
    "X_sample_rf, _, y_sample_rf, _ = train_test_split(X, y, stratify=y, train_size=0.2, random_state=42)  # Sample 20% of the data\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42, class_weight = 'balanced')\n",
    "param_distributions = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20],\n",
    "                       'min_samples_split': [2, 5],'min_samples_leaf': [1, 2], 'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "random_search = RandomizedSearchCV(rf, param_distributions, n_iter=10, scoring='roc_auc', cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_sample_rf, y_sample_rf)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best AUC-ROC score: {random_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now use best parameters from grid search cv on full dataset\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 200, min_samples_split = 5, min_samples_leaf = 1, max_features = 'log2', max_depth = 10,\n",
    "                                       random_state=42, class_weight = 'balanced', n_jobs = -1)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('accurary', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall', recall)\n",
    "print('f1', f1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Perform oversampling to address class imbalance\n",
    "oversampler = RandomOverSampler()\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_scaled, y)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(7,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define your model as before\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model as before\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize the ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model with the callback\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[reduce_lr] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"updated_feature_engineered_data.pkl\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_df = pd.read_csv(\"/Users/alyssaliu/Desktop/StatsM148/Event Definitions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['customer_id', 'account_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ideal_journey'] = df['ideal_journey'].map({0: 'unsuccessful', 1: 'successful'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['promotion_exposure'] = df['promotion_exposure'].map({0: 'no', 1: 'yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = {'Apply for Credit' : 1, 'Credit Account' : 2, 'Discover' : 3, 'Downpayment' : 4, 'First Purchase' : 5, \n",
    "              'Order Shipped' : 6, 'Prospecting' : 7}\n",
    "stage_dict = {v:k for k,v in stage_dict.items()}\n",
    "\n",
    "df['first_stage'] = df['first_stage'].map(stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ideal_journey'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['promotion_exposure'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='ideal_journey', y='days_since_start')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='days_since_start', hue='ideal_journey', kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'ideal_journey', y = 'days_since_start', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = df.groupby('ideal_journey')['promotion_exposure'].value_counts(normalize=True).rename('proportion').reset_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(x='ideal_journey', y='proportion', hue='promotion_exposure', data=proportions, errorbar = None)\n",
    "ax.bar_label(ax.containers[0], fontsize=10)\n",
    "plt.title('Normalized Count Plot by Target')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='first_stage', hue='ideal_journey', data=df, palette='Set2')\n",
    "plt.title('Distribution of First Stages by Ideal Journey Status')\n",
    "plt.xlabel('First Stage')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Ideal Journey', labels=['No', 'Yes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = df.groupby(['first_stage', 'ideal_journey']).size().unstack(fill_value=0)\n",
    "proportions_normalized = proportions.div(proportions.sum(axis=1), axis=0)\n",
    "proportions_normalized.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title('Normalized Distribution of First Stages by Ideal Journey Status')\n",
    "plt.xlabel('First Stage')\n",
    "plt.ylabel('Proportion')\n",
    "plt.axhline(y=0.183241, color='red', linestyle='--', label = 'true prop of successful/unsuccesful')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ideal = df[df['ideal_journey'] == 'successful']\n",
    "df_nonideal = df[df['ideal_journey'] == 'unsuccessful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "df_ideal['first_stage'].value_counts(normalize=True).plot(kind='bar')\n",
    "for i, v in enumerate(df_ideal['first_stage'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 6)), ha='center', va='bottom')\n",
    "plt.title(\"Proportion of first stage for customers with successful ideal journey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "df_nonideal['first_stage'].value_counts(normalize=True).plot(kind='bar')\n",
    "\n",
    "for i, v in enumerate(df_nonideal['first_stage'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 6)), ha='center', va='bottom')\n",
    "plt.title(\"Proportion of first stage for customers with unsuccessful ideal journey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = pd.read_pickle(\"/Users/alyssaliu/Desktop/StatsM148/cleaned_wide_format_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.set_index(['customer_id', 'account_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide_df['ideal_journey'] = df['ideal_journey']\n",
    "wide_df['ideal_journey'] = wide_df.index.map(df['ideal_journey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df['event_count'] = wide_df['event_name'].apply(len)\n",
    "wide_df['unique_event_count'] = wide_df['event_name'].apply(lambda x: len(set(x)))\n",
    "wide_df['unique_stage_count'] = wide_df['stage'].apply(lambda x: len(set(x)))\n",
    "wide_df['last_event'] = wide_df['event_name'].apply(lambda x: x[-1] if x else None)\n",
    "wide_df['last_stage'] = wide_df['stage'].apply(lambda x: x[-1] if x else None)\n",
    "wide_df['len_journey'] = wide_df['journey_steps_until_end'].apply(len)\n",
    "wide_df['days_in_journey'] = wide_df['event_timestamp'].apply(lambda x: (x[-1] - x[0]).days)\n",
    "wide_df['progression_rate'] = wide_df['len_journey'] / wide_df['days_in_journey'] # estimate for progression rate of journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df['first_purchase'] = wide_df['stage'].apply(lambda x: 1 if 'First Purchase' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id_dict = dict(zip(def_df['event_definition_id'], def_df['event_name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = def_df.set_index('event_definition_id')['stage'].to_dict()\n",
    "\n",
    "# add additional key value pair\n",
    "event_dict[1] = 'Promotion Created'\n",
    "event_dict[24] = 'Campaignemail Clicked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=wide_df, x='unique_event_count', hue='ideal_journey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=wide_df, x='unique_event_count', hue='ideal_journey')\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df = wide_df[wide_df['ideal_journey'] == 'successful']\n",
    "nonideal_df = wide_df[wide_df['ideal_journey'] == 'unsuccessful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = wide_df.groupby(['last_stage', 'ideal_journey']).size().unstack(fill_value=0)\n",
    "proportions_normalized = proportions.div(proportions.sum(axis=1), axis=0)\n",
    "proportions_normalized.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title('Normalized Distribution of Last Stages by Ideal Journey Status')\n",
    "plt.xlabel('Last Stage')\n",
    "plt.ylabel('Proportion')\n",
    "plt.axhline(y=0.183241, color='red', linestyle='--', label = 'true prop of successful/unsuccesful')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "ideal_df['last_stage'].value_counts(normalize=True).plot(kind='bar')\n",
    "for i, v in enumerate(ideal_df['last_stage'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 6)), ha='center', va='bottom')\n",
    "plt.title(\"Proportion of last stage for customers with successful ideal journey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "nonideal_df['last_event'].value_counts(normalize=True).plot(kind='bar')\n",
    "for i, v in enumerate(nonideal_df['last_event'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.title(\"Proportion of last events for customers with unsuccessful ideal journey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "nonideal_df['last_stage'].value_counts(normalize=True).plot(kind='bar')\n",
    "for i, v in enumerate(nonideal_df['last_stage'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 6)), ha='center', va='bottom')\n",
    "plt.title(\"Proportion of last stage for customers with unsuccessful ideal journey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.countplot(data=wide_df, x='last_stage', hue='ideal_journey')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = wide_df.groupby(['last_event', 'ideal_journey']).size().unstack(fill_value=0)\n",
    "proportions_normalized = proportions.div(proportions.sum(axis=1), axis=0)\n",
    "proportions_normalized.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title('Normalized Distribution of Last Event by Ideal Journey Status')\n",
    "plt.xlabel('Last Event')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=wide_df, x='ideal_journey', y='event_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stage_purchase = wide_df[wide_df['last_stage'] == 'First Purchase']\n",
    "last_stage_purchase['last_event'].value_counts(normalize = True).plot(kind = 'bar')\n",
    "for i, v in enumerate(last_stage_purchase['last_event'].value_counts(normalize=True)):\n",
    "    plt.text(i, v, str(round(v, 6)), ha='center', va='bottom')\n",
    "plt.title(\"Distribution of final event for customers with First Purchase as final stage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_purchase_df = wide_df[widf['stage'].apply(lambda stages: 'First Purchase' in stages)]\n",
    "event_names = list(chain.from_iterable(wide_df['event_name']))\n",
    "event_name_counts = pd.Series(event_names).value_counts()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=event_name_counts.values, y=event_name_counts.index)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Event Names')\n",
    "plt.title('Counts of Event Names Corresponding to \"First Purchase\" Stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=wide_df, x='len_journey', hue='ideal_journey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=wide_df, x='ideal_journey', y='len_journey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df['log_len_journey'] = np.log(wide_df['len_journey'] + 1) # log-transformed data\n",
    "sns.boxplot(data=wide_df, x='ideal_journey', y='log_len_journey')\n",
    "plt.title('Boxplot of log-transformed journey lengths')\n",
    "plt.ylabel('Log of Journey Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost & Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle((\"updated_feature_engineered_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.read_pickle(\"updated_clustering_sample_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df.drop(['customer_id', 'account_id'], axis = 1)\n",
    "X = model_df.drop(['ideal_journey'], axis = 1)\n",
    "y = model_df['ideal_journey'] # we use ideal_journey rather than order_shipped etc because the two variables have a correlation of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1] # scale weights of imbalanced data \n",
    "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid to search\n",
    "param_dist = {'n_estimators': randint(100, 500),'learning_rate': uniform(0.01, 0.3),'subsample': uniform(0.7, 0.3),'max_depth': randint(3, 10),\n",
    "              'colsample_bytree': uniform(0.7, 0.3),'min_child_weight': randint(1, 6)}\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=10, \n",
    "                                   scoring='roc_auc', error_score=0, verbose=3, n_jobs=-1, cv=3)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best ROC AUC found: {random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/Users/alyssaliu/Desktop/StatsM148/updated_feature_engineered_data.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['ideal_journey']\n",
    "X = df.drop(['customer_id', 'account_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 20): \n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0) # multiple initializations\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_) \n",
    "plt.plot(range(1, 20), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot = 'first_event'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='Cluster', y=feature_to_plot, data=df)\n",
    "plt.title(f'Distribution of {feature_to_plot} Across Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot = 'first_event_month'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='Cluster', y=feature_to_plot, data=df)\n",
    "plt.title(f'Distribution of {feature_to_plot} Across Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(hue='first_stage', x='Cluster', data=df)\n",
    "plt.title(f'Distribution of {feature_to_plot} Across Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=X_scaled.columns)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(centers, annot=True, cmap='viridis')\n",
    "plt.title('Centroid Values across Clusters')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = df[df['Cluster'] == 0]\n",
    "cluster_0_cust = cluster_0['customer_id'].to_list()\n",
    "print(cluster_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = df[df['Cluster'] == 1]\n",
    "cluster_1_cust = cluster_1['customer_id'].to_list()\n",
    "print(cluster_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2 = df[df['Cluster'] == 2]\n",
    "cluster_2_cust = cluster_2['customer_id'].to_list()\n",
    "print(cluster_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3 = df[df['Cluster'] == 3]\n",
    "cluster_3_cust = cluster_3['customer_id'].to_list()\n",
    "print(cluster_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4 = df[df['Cluster'] == 4]\n",
    "cluster_4_cust = cluster_4['customer_id'].to_list()\n",
    "print(cluster_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5 = df[df['Cluster'] == 5]\n",
    "cluster_5_cust = cluster_5['customer_id'].to_list()\n",
    "print(cluster_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting entire dataframe per cluster\n",
    "\n",
    "df_whole = pd.read_pickle(\"cleaned_wide_format_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_df = df_whole[df_whole['customer_id'].isin(cluster_1_cust)]\n",
    "cluster_2_df = df_whole[df_whole['customer_id'].isin(cluster_2_cust)]\n",
    "cluster_3_df = df_whole[df_whole['customer_id'].isin(cluster_3_cust)]\n",
    "cluster_4_df = df_whole[df_whole['customer_id'].isin(cluster_4_cust)]\n",
    "cluster_5_df = df_whole[df_whole['customer_id'].isin(cluster_5_cust)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = df_whole[df_whole['customer_id'].isin(cluster_0_cust)]\n",
    "cluster_0_df.to_pickle(\"cluster_0_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export cluster dataframes for easy access\n",
    "cluster_1_df.to_pickle(\"cluster_1_df.pkl\")\n",
    "cluster_2_df.to_pickle(\"cluster_2_df.pkl\")\n",
    "cluster_3_df.to_pickle(\"cluster_3_df.pkl\")\n",
    "cluster_4_df.to_pickle(\"cluster_4_df.pkl\")\n",
    "cluster_5_df.to_pickle(\"cluster_5_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_counts(df):\n",
    "    source_target_pairs = []\n",
    "    for path in df['stage']:\n",
    "        pairs = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "        source_target_pairs.extend(pairs)\n",
    "    pairs_df = pd.DataFrame(source_target_pairs, columns=['source', 'target'])\n",
    "    flow_counts = pairs_df.groupby(['source', 'target']).size().reset_index(name='value')\n",
    "    all_nodes = list(set(flow_counts['source']).union(set(flow_counts['target'])))\n",
    "    node_dict = {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "    # Map to df\n",
    "    flow_counts['source_id'] = flow_counts['source'].map(node_dict)\n",
    "    flow_counts['target_id'] = flow_counts['target'].map(node_dict)\n",
    "    \n",
    "    return flow_counts\n",
    "\n",
    "\n",
    "def create_flow_counts_events(df):\n",
    "    source_target_pairs = []\n",
    "    for path in df['event_name']:\n",
    "        pairs = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "        source_target_pairs.extend(pairs)\n",
    "    pairs_df = pd.DataFrame(source_target_pairs, columns=['source', 'target'])\n",
    "    flow_counts = pairs_df.groupby(['source', 'target']).size().reset_index(name='value')\n",
    "    all_nodes = list(set(flow_counts['source']).union(set(flow_counts['target'])))\n",
    "    node_dict = {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "    # Map to df\n",
    "    flow_counts['source_id'] = flow_counts['source'].map(node_dict)\n",
    "    flow_counts['target_id'] = flow_counts['target'].map(node_dict)\n",
    "    \n",
    "    return flow_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_1_flow_counts = create_flow_counts(cluster_1_df)\n",
    "clust_2_flow_counts = create_flow_counts(cluster_2_df)\n",
    "clust_3_flow_counts = create_flow_counts(cluster_3_df)\n",
    "clust_4_flow_counts = create_flow_counts(cluster_4_df)\n",
    "clust_5_flow_counts = create_flow_counts(cluster_5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_1_flow_counts[clust_1_flow_counts['source'] == clust_1_flow_counts['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_counts = create_flow_counts(df_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_flow_df = flow_counts.pivot_table(index='source', columns='target', values='value', aggfunc='sum')\n",
    "stage_flow_df = stage_flow_df.fillna(0)\n",
    "plt.figure(figsize=(12, 10)) \n",
    "sns.heatmap(stage_flow_df, annot=True, cmap='viridis', fmt='g', linewidths=.5)\n",
    "plt.title('Heatmap of Flow Count (for stages)')\n",
    "plt.ylabel('Source')\n",
    "plt.xlabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_counts[flow_counts['source'] == flow_counts['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flow_counts = create_flow_counts_events(df_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_df = pd.read_csv(\"event_definitions.csv\")\n",
    "first_purchase = list(def_df[def_df['stage'] == 'First Purchase']['event_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_purchase_flow_df = event_flow_counts[(event_flow_counts['source'].isin(first_purchase)) & (event_flow_counts['target'].isin(first_purchase))]\n",
    "# first_purchase_flow_df = first_purchase_flow_df[first_purchase_flow_df['source'] != first_purchase_flow_df['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_flow = first_purchase_flow_df.pivot_table(index='source', columns='target', values='value', aggfunc='sum')\n",
    "fp_flow = fp_flow.fillna(0)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(fp_flow, annot=True, cmap='viridis', fmt='g', linewidths=.5)\n",
    "plt.title('Heatmap of Flow Count (for first purchase events)')\n",
    "plt.ylabel('Source')\n",
    "plt.xlabel('Target')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
